---
title: "DATA621 HW5"
author: "Misha Kollontai"
date: "12/9/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE)
```


## Overview

In this homework assignment, you will explore, analyze and model a data set containing information on approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine distribution companies after sampling a wine. These cases would be used to provide tasting samples to restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a wine to be sold at a high end restaurant. A large wine manufacturer is studying the data in order to predict the number of wine cases ordered based upon the wine characteristics. If the wine manufacturer can predict the number of cases, then that manufacturer will be able to adjust their wine offering to maximize sales.

Your objective is to build a count regression model to predict the number of cases of wine that will be sold given certain properties of the wine. HINT: Sometimes, the fact that a variable is missing is actually predictive of the target. You can only use the variables given to you (or variables that you derive from the variables provided).



```{r,message=FALSE, echo = FALSE}
#load packages
library(knitr)
library(dplyr)
library(kableExtra)
library(stats)
library(corrplot)
library(psych)
library(dplyr)
library(ggplot2)
library(reshape2)
library(tidyr)
library(broom)
library(car)
library(pROC)
library(OneR)
library(MASS)
```
 



```{r,message=FALSE, echo = FALSE}
#load training data
url_train<- 'https://raw.githubusercontent.com/mkollontai/DATA621/main/HW%205/wine-training-data.csv'
wine_train_df <- read.csv(url_train, header = TRUE)
#wine_train_df$INCOME <- as.numeric(gsub(',','',substring(ins_train_df$INCOME,2)))
head(wine_train_df)

```

## Data Exploration

Let's calculate summary statistics and generate a box plots for further review.

```{r,echo=FALSE}
summary(wine_train_df)
```

There appear to be a significant amount of missing (NA) data. In order to see what effect each of our variables may have on our predictive model, let's take a look and see how the variables relate to the number of cases sold (our target variable). We will look at a spattering of the available variables. 

### Label Appeal

In order to evaluate the impact of the label appel, let's take a look at how many cases each "score" of label appeal sold per wine. Conventional knowledge suggests that more appealing labels will sell more cases. 

```{r}
lab_app <- wine_train_df %>%
  group_by(TARGET) %>%
  summarize(mean = mean(LabelAppeal, na.rm = TRUE))

ggplot(lab_app, aes(x = TARGET, y  = mean)) + 
  geom_point()+
  geom_smooth()
```


We can clearly see that the more appealing bottles sell more cases on average. This looks to be a very strong predictor of sales numbers. 

### Alcohol

Alcohol content is another variable we have at our disposal. Some peopl emay be looking for wine with a lower alcohol content, while others may prefer a stronger wine. Let's take a look at our data and see what trends present themselves. We can look at the average alcohol content for wines that sold a particular number of cases to identify possible relationships. We also know from above that we have over 650 NAs in the data that will need to be accounted for. 

```{r, echo = FALSE, warning=FALSE}
alc <- wine_train_df %>%
  group_by(TARGET) %>%
  summarize(mean = mean(Alcohol, na.rm = TRUE))

ggplot(alc, aes(x = TARGET, y  = mean)) + 
  geom_point()+
  geom_smooth()
```

Here we can see that as the number of cases sold increases, so does the average alcool content of the wines, though it must be noted that there is a sharp dropoff at 8 cases sold to the lowest average alcohol content in the set - perhaps errors in the data coupled with a small sample size? 

### Acidity

Now let's take a look into some of the acidity variables.

#### Fixed Acidity

```{r, echo = FALSE, warning=FALSE}
fix_acid <- wine_train_df %>%
  group_by(TARGET) %>%
  summarize(mean = mean(FixedAcidity, na.rm = TRUE))

ggplot(fix_acid, aes(x = TARGET, y  = mean)) + 
  geom_point()+
  geom_smooth()
```

Lower fixed acidity seems to correlate with higher cases sold. 

#### Volatile Acidity

```{r, echo = FALSE, warning=FALSE}
vol_acid <- wine_train_df %>%
  group_by(TARGET) %>%
  summarize(mean = mean(VolatileAcidity, na.rm = TRUE))

ggplot(vol_acid, aes(x = TARGET, y  = mean)) + 
  geom_point()+
  geom_smooth()
```

Volatile acidity seems to follow a similar trend as the fixed acidity, though for some reason there is a spike in sales again at 8 cases. Perhaps the low sample size for 8 cases sold is skewing our data.

#### Citric Acidity

```{r, echo = FALSE, warning=FALSE}
cit_acid <- wine_train_df %>%
  group_by(TARGET) %>%
  summarize(mean = mean(CitricAcid, na.rm = TRUE))

ggplot(cit_acid, aes(x = TARGET, y  = mean)) + 
  geom_point()+
  geom_smooth()
```

### Stars

The number of stars assigned to a bottle is likely to influence the sales numbers associated with it. 

```{r, echo = FALSE, warning=FALSE}
stars <- wine_train_df %>%
  group_by(TARGET) %>%
  summarize(mean = mean(STARS, na.rm = TRUE))

ggplot(stars, aes(x = TARGET, y  = mean)) + 
  geom_point()+
  geom_smooth()
```

This relationship is disturbingly obvious - to the point where it is difficult to believe the two variables are truly independent. 


---

## Data Preparation

### Imputation

What columns are missing data?

```{r, echo = FALSE}
colSums(is.na(wine_train_df))
```
As we can see, there is a large amount of data missing for one of our potentially strongest predictors - STARS. All in all, 8 of the 14 predictor variables have missing data. For the variable `STARS` there is no information for more than 25% of the entries. Chances are we will need to come up with some strong predictors for when the STARS rating isn't available. 

Let's take a look at each variable with missing data in turn to determine the best path forward for each. 

#### Residual Sugar Imputation

```{r, echo=FALSE}
hist(wine_train_df$ResidualSugar)
```

Here we see an intereseting picture - a somewhat normal distribution cenetered at 0, meaning a large portion of the data is negative. There is no such thing as a negative Residual Sugar levl, since it is measured in grams per Liter. One possibility is that the data reflects the delta from the mean or median of wines. If this assumption is correct, then assigning a value of 0 to NAs for this variable is a logcal way to go. 

#### Chlorides Imputation

```{r, echo=FALSE}
hist(wine_train_df$Chlorides)
```

The data for Chlorides follows a similar distribution to that of the Residal Sugars. We will impute in the same way. 

#### Free Sulfur Dioxide

```{r, echo=FALSE}
hist(wine_train_df$FreeSulfurDioxide)
```

#### Total Sulfur Dioxide

```{r, echo=FALSE}
hist(wine_train_df$TotalSulfurDioxide)
```

In this case the data doesn't appear to be centered at 0, so we will impute with the median. The fact that negative "Total Sulfur Dioxide" is being reported does raise concerns about the accuracy of the provided data. 

#### pH

```{r, echo=FALSE}
hist(wine_train_df$pH)
```

Similar to the total sulfur dioxide, the data is centered around a value other than 0 - we will impute accordingly. 

#### Sulphates

```{r, echo=FALSE}
hist(wine_train_df$Sulphates)
```

#### Alcohol

```{r, echo=FALSE}
hist(wine_train_df$Alcohol)
```

#### Acid Index

```{r, echo=FALSE}
hist(wine_train_df$AcidIndex)
```

This veriable isn't missing data, but it is interesting to see that is skewed right, with a majority of the wines having a lower index between 5 and 10. The hghest Acidity Index present in the data is `r max(wine_train_df$AcidIndex)`

#### Stars

```{r, echo=FALSE}
hist(wine_train_df$STARS)
```

This is a case of a variable where there are discrete values available and the data is not normally distributed. This makes it difficult to backfill missing data. What we will do is create a new column that tracks whether or not a STARS variable was present. If it was not, we will assign the mean value to the STARS column. 

````{r}
wine_train_df$ResidualSugar[is.na(wine_train_df$ResidualSugar)] <- 0
wine_train_df$Chlorides[is.na(wine_train_df$Chlorides)] <- 0
wine_train_df$FreeSulfurDioxide[is.na(wine_train_df$FreeSulfurDioxide)] <- 0
wine_train_df$TotalSulfurDioxide[is.na(wine_train_df$TotalSulfurDioxide)] <- median(wine_train_df$TotalSulfurDioxide, na.rm = TRUE)
wine_train_df$pH[is.na(wine_train_df$pH)] <- median(wine_train_df$pH, na.rm = TRUE)
wine_train_df$Sulphates[is.na(wine_train_df$Sulphates)] <- 0
wine_train_df$Alcohol[is.na(wine_train_df$Alcohol)] <- median(wine_train_df$Alcohol, na.rm = TRUE)
wine_train_df$Has_star <- ifelse(is.na(wine_train_df$STARS),0,1)
wine_train_df$STARS[is.na(wine_train_df$STARS)] <- mean(wine_train_df$STARS, na.rm = TRUE)
colSums(is.na(wine_train_df))
```

### Correlation

Do our variables correlate to the target variable at all?

```{r, echo = FALSE}
corr_df <- wine_train_df[,-1]
cor_wine_tr <- cor(corr_df, method="pearson") 
corrplot(cor_wine_tr)
```

The variables appear fairly independent of one another with only LabelAppeal, AcidIndex, STARS and whether it has a STAR or not showing a strong correlation with the TARGET variable. Interestingly, LabelAppeal and STARS show correlation, suggesting that one of those variables may influence the other. 

---

### Transforming Data

We created a new variable above, tracking whether or not there was STARS data available for each wine. If any other quirks in the data present themselves, we will create new variables. 

## Build Models

To start, let's create some Poisson Regression models to 

### Poisson Regressions

#### Model 1 - First Poisson Regression

```{r}
#build model 1 - all variables
wine_train_data <- wine_train_df[,-1]

model1 <- glm(TARGET ~., data = wine_train_data, family = 'poisson')
summary(model1)
```

As expected, we see that the strongest predictors are the LabelAppeal, AcidIndex, STARS and Has_Star variables. AcidIndex is the only of these 4 that has a negative coefficient, suggesting that a lower acid index is appealing to consumers. 

---

For our second model, let's reduce the number of less significant variables and trim the model somewhat by stepwise removing variables that have insignificant p-values. 

#### Model 2 - Trimmed Poisson Regression

```{r}
model2 <- glm(TARGET ~. -FixedAcidity -ResidualSugar -CitricAcid -Density, data = wine_train_data, family = poisson)
summary(model2)
```

### Negative Binomial Regressions

#### Model 3 - First Negative Binomial Regression

```{r}
#build model 3 - all variables negative binomial
model3 <- glm.nb(TARGET ~., data = wine_train_data)
summary(model3)
```

#### Model 4 - Second Negative Binomial Regression

```{r}
model4 <- glm.nb(TARGET ~.-FixedAcidity -ResidualSugar -CitricAcid -Density, data = wine_train_data)
summary(model4)
```

We can see here that our Poisson and Negative Binomial models yield nearly identical results - this is a result of the fact that the Poisson regression is in fact a subset of negative binomial regressions - one that assumes (the logarithm of its expected value can be modeled by a linear combination of unknown parameters)[https://en.wikipedia.org/wiki/Poisson_regression#:~:text=Poisson%20regression%20assumes%20the%20response,used%20to%20model%20contingency%20tables] 

### Multiple Linear Regressions

Now let's take a look at simple multiple linear regressions models using our available variables:

#### Model 5


```{r,message=FALSE}
model5 <- lm(TARGET ~., data = wine_train_data)
summary(model5)
```

```{r,message=FALSE}
model6 <- lm(TARGET ~.-FixedAcidity -ResidualSugar -CitricAcid - Density - pH - Sulphates, data = wine_train_data)
summary(model6)
```

## Select Models

Let's compare the distributions created by some of our models to the training data in order to evaluate which we will select as a final model.

```{r model compare, echo = FALSE}
model_df <- data.frame(Training = round(wine_train_df$TARGET),
                      Poisson1 = round(model1$fitted.values),
                      Poisson2 = round(model2$fitted.values))


dist_df <- gather(model_df,'Source', 'Cases') %>%
  group_by(Source) %>%
  count(Cases)

dist_df$Cases <- as.factor(dist_df$Cases)

ggplot(dist_df, aes(fill=Cases, y = n, x=Source)) + 
  geom_bar(position='stack', stat='identity') 
  #+  scale_fill_manual(values=palette(terrain.colors(11)))
```

We can see here that our models yield nearly no predictions where 0 cases are sold - this differs greatly from our traning dataset where a significant portion of wines sold 0 cases. There appears to be nearly no difference in the prediction values between our two Poisson models, showing that the impact of the removed variables is negligible. This supports our decision to do so. 

If we select the second, simplified model with an AIC of 45737, the distribution of predictions we see are:

```{r, echo = FALSE}
ggplot(dist_df[which(dist_df$Source=="Poisson2"),], aes(y = n, x=Cases)) + 
  geom_bar(stat='identity')
```